# Stop Level Data
You are an expert computer progrogrammer and data scientist. You want to take the data csv file that is a stop level data for a vehicle routing comapny, and you want to create visulizations of the stop level data in the script viz_stop_data.py, which will create an html file, stop_dashboard.html of the visulaizations. Have a constant at the top that points to the csv file, the current on is, data_geocode_20260203_30KNotFound.csv. Make the output similar to the route_dashboart.html. Below the title, include the number of unique stops, for unique stops, link them through the address_raw and the address_nominatim feature; so any stops that match in either of these are considered the same stop. Include a histogram of the number  Include a histogram of planned stop duration and the actual stop 

# Location Data
You are an expert computer progrogrammer and data scientist. You want to take the data csv file that is a stop level data for a vehicle routing comapny, and you create data about the locations. You will crate the python script create_location_data.py, which will ouput location_data.csv. First, you are going to find all the unique locations, my linking all stops through the address_raw and the address_nominatim. If either of these features match, than a stop is considered at that unique location. Create a unique stop ID for each stop, and note there could be multiple raw addresses for each stop, if they were entered differently into the system.

# Route Data
You are an expert computer progrogrammer and data scientist. You want to take the data csv file that is a stop level data for a vehicle routing comapny and conver it into route level data. Individual routes can generally be found by filtering by a specific date and a specific driver. So you will create a python script called create_route_data.py, that looks at each route, and creates the same summary statistics for each. Here are the metrics I would like to you to create. Here are the features I would like you to calculate and save in a csv, route_data.csv. The number_of_stops in the route. The first_stop_planned_time, look at the "Planned Time" of the "Planned Stop Number". Then similarly, the first_stop_actual_time, is "Actual_Time" of the first "Actual Stop Number". Then create a last_stop_actual_time and last_stop_planned_time in a similar manner. Create a total_planned_stop_duation, whic is the sum of the planned stop duration and a total_actual_stop_duration, which is the sum of the "actual duration". Next, create helper function that can recieve a list of latitude and longitudes and calculate the distance. Then calculate "route_distance_straight_line_planned" based on the availile latidudes and longidues and  "Planned Stop Number".  Then calculate "route_distance_straight_line_planned" based on the availile latidudes and longidues and  "Planned Stop Number". Then create a helper function that accepts two lists of integers, which represent the planned stop number and actual stop number. Determine how many of the edges were executed as planned. A quick way to calculate this is for each node, determine if the predecessor is the same in both lists.

# Commit and Push
Ok, can you make a commit, create a short description, and push to remote at https://github.com/stevenblum/bbbs.git

# Nominatim Geocode Lookup
You are an experienced coder. You are building an app the optimizes vehicle routing. You have an existing data set that you need to clean. One of the steps is to geocode the stop data, transfroming raw addresses that a scheduler has recorded into latitude and longitude. The nominatim library is runnin on a docker server on port 8080, ready for use. The c library libpostal is installed with the python bindings. in the clean_data folder, the data_add_geocode.py is a script that attemps to geocode the locations. Starting with the agg_data.csv and eventually saving a data_geocode.csv, geocode_address_cache.csv, and geocode_report.csv. Right now, the code goes through several different search attempts, which I dont think is optimal. Instead, I would like to just to try on search. Please adjust is so that it immeditaly parses the raw address with libpostal. Then for the serach it it should be structures like f"{unit}, {road}, {city}, {state}". This will by default become the address_cache. Please update the script to reflect this.